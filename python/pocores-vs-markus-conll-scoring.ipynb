{
 "metadata": {
  "name": "",
  "signature": "sha256:c685ad269cde758e1e606fa37565c0bd060343a61e446d933b9164089028c7d9"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import glob\n",
      "import tempfile\n",
      "import sh\n",
      "\n",
      "import discoursegraphs as dg\n",
      "from discoursegraphs.readwrite import MMAXDocumentGraph, write_conll\n",
      "from discoursekernels.util import draw_multiple_graphs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext gvmagic\n",
      "from discoursegraphs import print_dot"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The gvmagic extension is already loaded. To reload it, use:\n",
        "  %reload_ext gvmagic\n"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# wget http://www.ling.uni-potsdam.de/acl-lab/Forsch/pcc/potsdam-commentary-corpus-2.0.0.zip\n",
      "# unzip potsdam-commentary-corpus-2.0.0.zip -d ~/corpora"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "MMAX_DIR = os.path.expanduser('~/repos/pcc-annis-merged/maz176/coreference')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# grab a copy of the official CoNLL scorer using git-svn instead of svn\n",
      "# git svn clone http://reference-coreference-scorers.googlecode.com/svn/trunk/ ~/repos/reference-coreference-scorers"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "SCORER_PATH = '/home/arne/repos/reference-coreference-scorers/scorer.pl'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "scorer = sh.Command(SCORER_PATH)\n",
      "scorer()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 64,
       "text": [
        "version: 8.01 /home/arne/repos/reference-coreference-scorers/lib/CorScorer.pm\n",
        "\n",
        "use: scorer.pl <metric> <keys_file> <response_file> [name]\n",
        "\n",
        "  metric: the metric desired to score the results:\n",
        "    muc: MUCScorer (Vilain et al, 1995)\n",
        "    bcub: B-Cubed (Bagga and Baldwin, 1998)\n",
        "    ceafm: CEAF (Luo et al, 2005) using mention-based similarity\n",
        "    ceafe: CEAF (Luo et al, 2005) using entity-based similarity\n",
        "    blanc: BLANC\n",
        "    all: uses all the metrics to score\n",
        "\n",
        "  keys_file: file with expected coreference chains in SemEval format\n",
        "\n",
        "  response_file: file with output of coreference system (SemEval format)\n",
        "\n",
        "  name: [optional] the name of the document to score. If name is not\n",
        "    given, all the documents in the dataset will be scored. If given\n",
        "    name is \"none\" then all the documents are scored but only total\n",
        "    results are shown.\n"
       ]
      }
     ],
     "prompt_number": 64
    },
    {
     "cell_type": "heading",
     "level": 1,
     "metadata": {},
     "source": [
      "Compare CoNLL files against themselves"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We're now using the official CoNLL scorer to compare each  \n",
      "MAZ176 coreference annotated document against itself.  \n",
      "All comparisons should result in an F1 of 100%"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import sys\n",
      "\n",
      "def has_valid_annotation(mmax_file, scorer_path, metric, verbose=False):\n",
      "    \"\"\"\n",
      "    Parameters\n",
      "    ----------\n",
      "    metric : str\n",
      "        muc, bcub, ceafm, ceafe, blanc\n",
      "    verbose : bool or str\n",
      "        True, False or 'very'\n",
      "    \"\"\"\n",
      "    scorer = sh.Command(scorer_path)\n",
      "    mdg = MMAXDocumentGraph(mmax_file)\n",
      "    conll_fname = '/tmp/{}.conll'.format(os.path.basename(mmax_file))\n",
      "    write_conll(mdg, conll_fname)\n",
      "    try:\n",
      "        results = scorer(metric, conll_fname, conll_fname)\n",
      "        scores_str = results.stdout.splitlines()[-2]\n",
      "        if not scores_str.endswith('100%'):\n",
      "            if verbose == 'very':\n",
      "                sys.stderr.write(\"{}\\n{}\\n\".format(conll_fname, results))\n",
      "            elif verbose:\n",
      "                sys.stderr.write(\"{}\\n{}\\n\".format(conll_fname, scores_str))\n",
      "\n",
      "            return False\n",
      "                \n",
      "    except sh.ErrorReturnCode as e:\n",
      "        if verbose:\n",
      "            sys.stderr.write(\"Error in '{}'\\n{}\".format(conll_fname, e))\n",
      "        return False\n",
      "    return True\n",
      "\n",
      "\n",
      "def get_bad_scoring_files(mmax_dir, scorer_path, metric, verbose=False):\n",
      "    \"\"\"\n",
      "    returns filepaths of MMAX2 coreference files which don't produce perfect\n",
      "    results when testing them against themselves with scorer.pl\n",
      "    \"\"\"\n",
      "    bad_files = []\n",
      "    for mmax_file in glob.glob(os.path.join(mmax_dir, '*.mmax')):\n",
      "        if not has_valid_annotation(mmax_file, scorer_path, metric, verbose=verbose):\n",
      "            bad_files.append(mmax_file)\n",
      "    return bad_files"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "blanc_errors = get_bad_scoring_files(MMAX_DIR, SCORER_PATH, 'blanc', verbose=True)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/tmp/maz-9884.mmax.conll\n",
        "BLANC: Recall: (0.965336804537853 / 1) 96.53%\tPrecision: (1 / 1) 100%\tF1: 98.21%\n",
        "/tmp/maz-11735.mmax.conll\n",
        "BLANC: Recall: (0.998168498168498 / 1) 99.81%\tPrecision: (1 / 1) 100%\tF1: 99.9%\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/tmp/maz-15347.mmax.conll\n",
        "BLANC: Recall: (0.9989816700611 / 1) 99.89%\tPrecision: (1 / 1) 100%\tF1: 99.94%\n",
        "/tmp/maz-5012.mmax.conll\n",
        "BLANC: Recall: (0.997727272727273 / 1) 99.77%\tPrecision: (1 / 1) 100%\tF1: 99.88%\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/tmp/maz-17953.mmax.conll\n",
        "BLANC: Recall: (0.991241039426523 / 1) 99.12%\tPrecision: (1 / 1) 100%\tF1: 99.55%\n",
        "/tmp/maz-4472.mmax.conll\n",
        "BLANC: Recall: (0.999315068493151 / 1) 99.93%\tPrecision: (1 / 1) 100%\tF1: 99.96%\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/tmp/maz-5297.mmax.conll\n",
        "BLANC: Recall: (0.998806682577566 / 1) 99.88%\tPrecision: (1 / 1) 100%\tF1: 99.94%\n",
        "/tmp/maz-4282.mmax.conll\n",
        "BLANC: Recall: (0.957070707070707 / 1) 95.7%\tPrecision: (1 / 1) 100%\tF1: 97.76%\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/tmp/maz-12383.mmax.conll\n",
        "BLANC: Recall: (0.998820754716981 / 1) 99.88%\tPrecision: (1 / 1) 100%\tF1: 99.94%\n",
        "/tmp/maz-8134.mmax.conll\n",
        "BLANC: Recall: (0.999399759903962 / 1) 99.93%\tPrecision: (1 / 1) 100%\tF1: 99.96%\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/tmp/maz-8361.mmax.conll\n",
        "BLANC: Recall: (0.999300699300699 / 1) 99.93%\tPrecision: (1 / 1) 100%\tF1: 99.96%\n",
        "/tmp/maz-3367.mmax.conll\n",
        "BLANC: Recall: (0.999040307101727 / 1) 99.9%\tPrecision: (1 / 1) 100%\tF1: 99.95%\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/tmp/maz-3415.mmax.conll\n",
        "BLANC: Recall: (0.99966078697422 / 1) 99.96%\tPrecision: (1 / 1) 100%\tF1: 99.98%\n",
        "/tmp/maz-11299.mmax.conll\n",
        "BLANC: Recall: (0.999518768046198 / 1) 99.95%\tPrecision: (1 / 1) 100%\tF1: 99.97%\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/tmp/maz-14590.mmax.conll\n",
        "BLANC: Recall: (0.99803536345776 / 1) 99.8%\tPrecision: (1 / 1) 100%\tF1: 99.9%\n",
        "/tmp/maz-15734.mmax.conll\n",
        "BLANC: Recall: (0.998756218905473 / 1) 99.87%\tPrecision: (1 / 1) 100%\tF1: 99.93%\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/tmp/maz-12510.mmax.conll\n",
        "BLANC: Recall: (0.998296422487223 / 1) 99.82%\tPrecision: (1 / 1) 100%\tF1: 99.91%\n",
        "/tmp/maz-14047.mmax.conll\n",
        "BLANC: Recall: (0.999248120300752 / 1) 99.92%\tPrecision: (1 / 1) 100%\tF1: 99.96%\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/tmp/maz-17254.mmax.conll\n",
        "BLANC: Recall: (0.979511867442902 / 1) 97.95%\tPrecision: (1 / 1) 100%\tF1: 98.95%\n",
        "/tmp/maz-6993.mmax.conll\n",
        "BLANC: Recall: (0.996495327102804 / 1) 99.64%\tPrecision: (1 / 1) 100%\tF1: 99.82%\n"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "heading",
     "level": 3,
     "metadata": {},
     "source": [
      "Do all metrics choke on the same files?"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bad_scoring_files = {}\n",
      "for metric in ('muc', 'bcub', 'ceafm', 'ceafe', 'blanc'):\n",
      "    bad_scoring_files[metric] = get_bad_scoring_files(MMAX_DIR, SCORER_PATH, metric, verbose=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for metric in bad_scoring_files:\n",
      "    print \"number of erroneous files found by '{}': {}\".format(metric, len(bad_scoring_files[metric]))\n",
      "\n",
      "all_bad_files = set()\n",
      "for metric in bad_scoring_files:\n",
      "    all_bad_files.update(bad_scoring_files[metric])\n",
      "\n",
      "print \"total number of erroneous files:\", len(all_bad_files)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "number of erroneous files found by 'bcub': 20\n",
        "number of erroneous files found by 'ceafe': 20\n",
        "number of erroneous files found by 'ceafm': 20\n",
        "number of erroneous files found by 'muc': 6\n",
        "number of erroneous files found by 'blanc': 20\n",
        "total number of erroneous files: 20\n"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from discoursegraphs import get_pointing_chains"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from discoursegraphs.readwrite.mmax2 import spanstring2text\n",
      "\n",
      "def print_all_chains(docgraph):\n",
      "    \"\"\"\n",
      "    print a list of all pointing chains (i.e coreference chains)\n",
      "    contained in a document graph\n",
      "    \"\"\"\n",
      "    for chain in get_pointing_chains(docgraph):\n",
      "        for node_id in chain:\n",
      "            print node_id, spanstring2text(docgraph, docgraph.node[node_id][docgraph.ns+':span'])\n",
      "        print '\\n'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mdg = MMAXDocumentGraph(os.path.join(MMAX_DIR, 'maz-3377.mmax'))\n",
      "print_all_chains(mdg)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "markable_16 dieser\n",
        "markable_15 den B\u00fcrgermeister\n",
        "\n",
        "\n",
        "markable_14 sie\n",
        "markable_13 sie\n",
        "markable_9 die SPD-Fraktion\n",
        "markable_3 alle\n",
        "markable_1 unter den Dallgower Kommunalpolitikern\n",
        "\n",
        "\n",
        "markable_38 sie\n",
        "markable_36 Die anderen\n",
        "\n",
        "\n",
        "markable_29 der Freien W\u00e4hlergemeinschaft\n",
        "markable_30 eine Fraktion\n",
        "\n",
        "\n",
        "markable_28 ihrer\n",
        "markable_26 sie\n",
        "markable_20 die Christdemokraten\n",
        "markable_19 die CDU\n",
        "\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 71
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Hypothesis 1: all markables occurring in more than one coreference chain produce scoring errors"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from itertools import combinations\n",
      "\n",
      "def get_ambiguous_markables(mmax_docgraph):\n",
      "    \"\"\"returns a list of markables that occur in more than one coreference chain\"\"\"\n",
      "    ambiguous_markables = []\n",
      "    chain_sets = (set(chain) for chain in get_pointing_chains(mmax_docgraph))\n",
      "    for chain1, chain2 in combinations(chain_sets, 2):\n",
      "        chain_intersect = chain1.intersection(chain2)\n",
      "        if chain_intersect:\n",
      "            ambiguous_markables.extend(chain_intersect)\n",
      "    return ambiguous_markables"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 72
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "files_with_ambigious_chains = []\n",
      "for mmax_file in glob.glob(os.path.join(MMAX_DIR, '*.mmax')):\n",
      "    mdg = MMAXDocumentGraph(mmax_file)\n",
      "    if get_ambiguous_markables(mdg):\n",
      "        files_with_ambigious_chains.append(mmax_file)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"# of files with ambiguous coreference chains: \", len(files_with_ambigious_chains)\n",
      "print \"# of files scorer.pl doesn't like: \", len(bad_scoring_files)\n",
      "if len(files_with_ambigious_chains) > 0:\n",
      "    print \"percent of files w/ ambiguous chains that scorer.pl doesn't like:\", \\\n",
      "        len( set(files_with_ambigious_chains).intersection(set(bad_scoring_files)) ) / len(files_with_ambigious_chains) * 100\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "# of files with ambiguous coreference chains:  0\n",
        "# of files scorer.pl doesn't like:  5\n"
       ]
      }
     ],
     "prompt_number": 74
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Initially, this was true. After Markus fixed a bunch of annotations,  \n",
      "Hypothesis 1 could not be validated any longer.**"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Hypothesis 2: non-contiguous markables cause trouble"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# test this with\n",
      "# markable_32 auf beiden Seiten\n",
      "# markable_56 Arafat Scharon\n",
      "\n",
      "\n",
      "mdg = MMAXDocumentGraph(os.path.join(MMAX_DIR, 'maz-19074.mmax'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mdg.node['markable_56']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 76,
       "text": [
        "{'label': 'markable_56:groups',\n",
        " 'layers': {'mmax',\n",
        "  'mmax:groups',\n",
        "  'mmax:markable',\n",
        "  'mmax:primmark',\n",
        "  'mmax:secmark'},\n",
        " 'mmax:id': 'markable_56',\n",
        " 'mmax:mmax_level': 'groups',\n",
        " 'mmax:span': 'word_42,word_45'}"
       ]
      }
     ],
     "prompt_number": 76
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## potential error\n",
      "\n",
      "* markable is both primmark and secmark"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from discoursegraphs import get_span, select_nodes_by_layer\n",
      "\n",
      "def get_noncontiguous_markables(docgraph):\n",
      "    \"\"\"return all markables that don't represent adjacent tokens\"\"\"\n",
      "    noncontiguous_markables = []\n",
      "    id2index = {tok_id:i for i, tok_id in enumerate(docgraph.tokens)}\n",
      "    for markable in select_nodes_by_layer(docgraph, docgraph.ns+':markable'):\n",
      "        span_token_ids = get_span(docgraph, markable)\n",
      "        for span_index, tok_id in enumerate(span_token_ids[:-1]):\n",
      "            tok_index = id2index[tok_id]\n",
      "            next_tok_id = span_token_ids[span_index+1]\n",
      "            next_tok_index = id2index[next_tok_id]\n",
      "            if next_tok_index - tok_index != 1:\n",
      "                noncontiguous_markables.append(markable)\n",
      "    return noncontiguous_markables"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "files_with_noncontiguous_markables = []\n",
      "for mmax_file in glob.glob(os.path.join(MMAX_DIR, '*.mmax')):\n",
      "    mdg = MMAXDocumentGraph(mmax_file)\n",
      "    if get_noncontiguous_markables(mdg):\n",
      "        files_with_noncontiguous_markables.append(mmax_file)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 78
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"# of files with non-continuous markables: \", len(files_with_noncontiguous_markables)\n",
      "print \"# of files scorer.pl doesn't like: \", len(bad_scoring_files)\n",
      "print \"percent of files w/ non-continuous markables that scorer.pl doesn't like:\", \\\n",
      "    len( set(files_with_noncontiguous_markables).intersection(set(bad_scoring_files)) ) / len(files_with_noncontiguous_markables) * 100\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "# of files with non-continuous markables:  133\n",
        "# of files scorer.pl doesn't like:  5\n",
        "percent of files w/ non-continuous markables that scorer.pl doesn't like: 0\n"
       ]
      }
     ],
     "prompt_number": 79
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Hypothesis 2 doesn't hold.**"
     ]
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Let's check files w/out ambiguous coreference chains that scorer.pl doesn't like"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "mysterious_files = [os.path.basename(fname)\n",
      "                    for fname in set(all_bad_files).difference(set(files_with_ambigious_chains))]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 80
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(mysterious_files)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 81,
       "text": [
        "20"
       ]
      }
     ],
     "prompt_number": 81
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for fname in mysterious_files:\n",
      "#     mdg = MMAXDocumentGraph(os.path.join(MMAX_DIR, fname))\n",
      "#     print fname, '\\n==============\\n\\n'\n",
      "#     try:\n",
      "#         print_all_chains(mdg)\n",
      "#     except Exception as e:\n",
      "#         print \"\\n{} FAILED: {}\".format(fname, e)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 92
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Visualizing ambiguous coreference annotations with discoursegraphs\n",
      "\n",
      "### fortunately, the current version doesn't have any"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from collections import defaultdict\n",
      "import networkx as nx\n",
      "\n",
      "from discoursegraphs import get_text\n",
      "\n",
      "def get_ambiguous_chains(mmax_docgraph, token_labels=False):\n",
      "    \"\"\"\n",
      "    Returns a list of networkx graphs that represent ambiguous\n",
      "    coreference chains. An ambiguous chain represents two or more\n",
      "    coreference chains that share at least one markable.\n",
      "    \n",
      "    There should be no ambiguous coreference chains, but the\n",
      "    current version of our annotation guidelines allow them. // SRSLY?\n",
      "    \"\"\"\n",
      "    ambiguous_markables = get_ambiguous_markables(mmax_docgraph)    \n",
      "    coreference_chains = get_pointing_chains(mmax_docgraph)\n",
      "    markable2chain = defaultdict(list)\n",
      "    for i, chain in enumerate(coreference_chains):\n",
      "        for markable in chain:\n",
      "            if markable in ambiguous_markables:\n",
      "                markable2chain[markable].append(i)\n",
      "    \n",
      "    chain_graphs = []\n",
      "    for markable in markable2chain:\n",
      "        ambig_chain_ids = markable2chain[markable]\n",
      "        chain_graph = nx.MultiDiGraph()\n",
      "        chain_graph.name = mmax_docgraph.name\n",
      "        for chain_id in ambig_chain_ids:\n",
      "            ambig_chain = coreference_chains[chain_id]\n",
      "            for i, markable in enumerate(ambig_chain[:-1]):\n",
      "                chain_graph.add_edge(markable, ambig_chain[i+1])\n",
      "\n",
      "        if token_labels:\n",
      "            for markable in chain_graph.nodes_iter():\n",
      "                markable_text = get_text(mmax_docgraph, markable)\n",
      "                chain_graph.node[markable]['label'] = markable_text\n",
      "        \n",
      "        chain_graphs.append(chain_graph)\n",
      "    return chain_graphs"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 83
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def merge_ambiguous_chains(ambiguous_chains):\n",
      "    \"\"\"\n",
      "    Parameters\n",
      "    ----------\n",
      "    ambiguous_chains : list of MultiDiGraph\n",
      "        a list of graphs, each representing an ambiguous coreference chain\n",
      "    \"\"\"\n",
      "    merged_chain = nx.DiGraph(nx.compose_all(ambiguous_chains))\n",
      "    merged_chain.add_node('name', shape='tab',\n",
      "                          color='blue',\n",
      "                          label=ambiguous_chains[0].name)\n",
      "    for node in merged_chain:\n",
      "        if merged_chain.in_degree(node) > 1 \\\n",
      "        or merged_chain.out_degree(node) > 1:\n",
      "            merged_chain.node[node]['color'] = 'red'\n",
      "    return merged_chain"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 84
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(files_with_ambigious_chains) # nothing to see here, move on!"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 85,
       "text": [
        "0"
       ]
      }
     ],
     "prompt_number": 85
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Are there any files without chains? no!"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from discoursegraphs import info\n",
      "\n",
      "files_without_chains = []\n",
      "for mmax_file in glob.glob(os.path.join(MMAX_DIR, '*.mmax')):\n",
      "    mdg = MMAXDocumentGraph(mmax_file)\n",
      "    if not get_pointing_chains(mdg):\n",
      "        files_without_chains.append(os.path.basename(mmax_file))\n",
      "#         info(mdg)\n",
      "#         print '\\n\\n'\n",
      "        \n",
      "print files_without_chains"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[]\n"
       ]
      }
     ],
     "prompt_number": 86
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "What's wrong with the remaining files?"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "\n",
      "*  files that produces an F1 of less than 100%\n",
      "* most of them are just off by one allegedly 'invented' entity"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for fname in mysterious_files:\n",
      "    has_valid_annotation(os.path.join(MMAX_DIR, fname), SCORER_PATH, 'muc', verbose='very')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/tmp/maz-6993.mmax.conll\n",
        "version: 8.01 /home/arne/repos/reference-coreference-scorers/lib/CorScorer.pm\n",
        "====> (__); __:\n",
        "File (__); __:\n",
        "Entity 0: (3,3) (73,76) (154,154)\n",
        "Entity 1: (12,12)\n",
        "Entity 2: (15,24)\n",
        "Entity 3: (16,22) (139,147)\n",
        "Entity 4: (20,22)\n",
        "Entity 5: (31,32) (49,50) (109,110)\n",
        "Entity 6: (39,39) (151,154)\n",
        "Entity 7: (45,50) (106,110)\n",
        "Entity 8: (54,59)\n",
        "Entity 9: (63,64)\n",
        "Entity 10: (69,76)\n",
        "Entity 11: (79,80) (92,92)\n",
        "Entity 12: (82,94)\n",
        "Entity 13: (96,96)\n",
        "Entity 14: (104,110)\n",
        "Entity 15: (115,116)\n",
        "Entity 16: (127,134)\n",
        "Entity 17: (130,130)\n",
        "Entity 18: (132,132)\n",
        "Entity 19: (134,134)\n",
        "Entity 20: (136,154)\n",
        "Entity 21: (136,154)\n",
        "Entity 22: (139,147)\n",
        "Entity 23: (139,147)\n",
        "Entity 24: (145,147)\n",
        "====> (__); __:\n",
        "File (__); __:\n",
        "Entity 0: (3,3) (73,76) (154,154)\n",
        "Entity 1: (12,12)\n",
        "Entity 2: (15,24)\n",
        "Entity 3: (16,22) (139,147)\n",
        "Entity 4: (20,22)\n",
        "Entity 5: (31,32) (49,50) (109,110)\n",
        "Entity 6: (39,39) (151,154)\n",
        "Entity 7: (45,50) (106,110)\n",
        "Entity 8: (54,59)\n",
        "Entity 9: (63,64)\n",
        "Entity 10: (69,76)\n",
        "Entity 11: (79,80) (92,92)\n",
        "Entity 12: (82,94)\n",
        "Entity 13: (96,96)\n",
        "Entity 14: (104,110)\n",
        "Entity 15: (115,116)\n",
        "Entity 16: (127,134)\n",
        "Entity 17: (130,130)\n",
        "Entity 18: (132,132)\n",
        "Entity 19: (134,134)\n",
        "Entity 20: (136,154)\n",
        "Entity 21: (136,154)\n",
        "Entity 22: (139,147)\n",
        "Entity 23: (139,147)\n",
        "Entity 24: (145,147)\n",
        "(__); __:\n",
        "Repeated mention in the key: 136, 154 2829\n",
        "Repeated mention in the key: 139, 147 630\n",
        "Repeated mention in the key: 139, 147 3031\n",
        "Repeated mention in the response: 136, 154 2929\n",
        "Repeated mention in the response: 139, 147 3131\n",
        "Repeated mention in the response: 139, 147 3131\n",
        "Total key mentions: 30\n",
        "Total response mentions: 30\n",
        "Strictly correct identified mentions: 30\n",
        "Partially correct identified mentions: 0\n",
        "No identified: 0\n",
        "Invented: 3\n",
        "Recall: (7 / 8) 87.5%\tPrecision: (7 / 8) 87.5%\tF1: 87.5%\n",
        "--------------------------------------------------------------------------\n",
        "\n",
        "====== TOTALS =======\n",
        "Identification of Mentions: Recall: (30 / 30) 100%\tPrecision: (30 / 30) 100%\tF1: 100%\n",
        "--------------------------------------------------------------------------\n",
        "Coreference: Recall: (7 / 8) 87.5%\tPrecision: (7 / 8) 87.5%\tF1: 87.5%\n",
        "--------------------------------------------------------------------------\n",
        "\n",
        "/tmp/maz-17254.mmax.conll\n",
        "version: 8.01 /home/arne/repos/reference-coreference-scorers/lib/CorScorer.pm\n",
        "====> (__); __:\n",
        "File (__); __:\n",
        "Entity 0: (1,8)\n",
        "Entity 1: (4,6) (41,42) (121,121) (179,179)\n",
        "Entity 2: (4,8) (64,64) (92,92) (126,126) (137,137) (161,161)\n",
        "Entity 3: (4,8) (46,47)\n",
        "Entity 4: (11,20)\n",
        "Entity 5: (16,18) (32,33) (48,48)\n",
        "Entity 6: (29,33)\n",
        "Entity 7: (37,47)\n",
        "Entity 8: (39,42)\n",
        "Entity 9: (43,47)\n",
        "Entity 10: (53,55)\n",
        "Entity 11: (62,65)\n",
        "Entity 12: (66,68)\n",
        "Entity 13: (74,76) (78,78)\n",
        "Entity 14: (77,79) (108,110)\n",
        "Entity 15: (84,84)\n",
        "Entity 16: (86,90)\n",
        "Entity 17: (95,96)\n",
        "Entity 18: (101,105)\n",
        "Entity 19: (121,123) (130,130)\n",
        "Entity 20: (127,128)\n",
        "Entity 21: (129,131)\n",
        "Entity 22: (141,142)\n",
        "Entity 23: (150,156) (158,158)\n",
        "Entity 24: (154,154)\n",
        "Entity 25: (156,156)\n",
        "Entity 26: (169,175)\n",
        "Entity 27: (173,173)\n",
        "Entity 28: (175,175)\n",
        "Entity 29: (178,180)\n",
        "====> (__); __:\n",
        "File (__); __:\n",
        "Entity 0: (1,8)\n",
        "Entity 1: (4,6) (41,42) (121,121) (179,179)\n",
        "Entity 2: (4,8) (64,64) (92,92) (126,126) (137,137) (161,161)\n",
        "Entity 3: (4,8) (46,47)\n",
        "Entity 4: (11,20)\n",
        "Entity 5: (16,18) (32,33) (48,48)\n",
        "Entity 6: (29,33)\n",
        "Entity 7: (37,47)\n",
        "Entity 8: (39,42)\n",
        "Entity 9: (43,47)\n",
        "Entity 10: (53,55)\n",
        "Entity 11: (62,65)\n",
        "Entity 12: (66,68)\n",
        "Entity 13: (74,76) (78,78)\n",
        "Entity 14: (77,79) (108,110)\n",
        "Entity 15: (84,84)\n",
        "Entity 16: (86,90)\n",
        "Entity 17: (95,96)\n",
        "Entity 18: (101,105)\n",
        "Entity 19: (121,123) (130,130)\n",
        "Entity 20: (127,128)\n",
        "Entity 21: (129,131)\n",
        "Entity 22: (141,142)\n",
        "Entity 23: (150,156) (158,158)\n",
        "Entity 24: (154,154)\n",
        "Entity 25: (156,156)\n",
        "Entity 26: (169,175)\n",
        "Entity 27: (173,173)\n",
        "Entity 28: (175,175)\n",
        "Entity 29: (178,180)\n",
        "(__); __:\n",
        "Repeated mention in the key: 4, 8 511\n",
        "Repeated mention in the response: 4, 8 1111\n",
        "Total key mentions: 44\n",
        "Total response mentions: 44\n",
        "Strictly correct identified mentions: 44\n",
        "Partially correct identified mentions: 0\n",
        "No identified: 0\n",
        "Invented: 1\n",
        "Recall: (13 / 15) 86.66%\tPrecision: (13 / 14) 92.85%\tF1: 89.65%\n",
        "--------------------------------------------------------------------------\n",
        "\n",
        "====== TOTALS =======\n",
        "Identification of Mentions: Recall: (44 / 44) 100%\tPrecision: (44 / 44) 100%\tF1: 100%\n",
        "--------------------------------------------------------------------------\n",
        "Coreference: Recall: (13 / 15) 86.66%\tPrecision: (13 / 14) 92.85%\tF1: 89.65%\n",
        "--------------------------------------------------------------------------\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/tmp/maz-4282.mmax.conll\n",
        "version: 8.01 /home/arne/repos/reference-coreference-scorers/lib/CorScorer.pm\n",
        "====> (__); __:\n",
        "File (__); __:\n",
        "Entity 0: (8,10)\n",
        "Entity 1: (9,9) (61,61)\n",
        "Entity 2: (9,43) (127,130)\n",
        "Entity 3: (15,20)\n",
        "Entity 4: (19,20) (114,117)\n",
        "Entity 5: (25,27)\n",
        "Entity 6: (28,31) (44,44)\n",
        "Entity 7: (28,46) (90,91)\n",
        "Entity 8: (39,43)\n",
        "Entity 9: (39,43) (65,66)\n",
        "Entity 10: (39,43)\n",
        "Entity 11: (60,62)\n",
        "Entity 12: (65,69) (85,85) (89,89)\n",
        "Entity 13: (71,72)\n",
        "Entity 14: (99,100)\n",
        "Entity 15: (105,108)\n",
        "Entity 16: (125,126) (134,134) (141,141)\n",
        "Entity 17: (147,148)\n",
        "Entity 18: (153,153)\n",
        "Entity 19: (161,163)\n",
        "Entity 20: (171,173)\n",
        "====> (__); __:\n",
        "File (__); __:\n",
        "Entity 0: (8,10)\n",
        "Entity 1: (9,9) (61,61)\n",
        "Entity 2: (9,43) (127,130)\n",
        "Entity 3: (15,20)\n",
        "Entity 4: (19,20) (114,117)\n",
        "Entity 5: (25,27)\n",
        "Entity 6: (28,31) (44,44)\n",
        "Entity 7: (28,46) (90,91)\n",
        "Entity 8: (39,43)\n",
        "Entity 9: (39,43) (65,66)\n",
        "Entity 10: (39,43)\n",
        "Entity 11: (60,62)\n",
        "Entity 12: (65,69) (85,85) (89,89)\n",
        "Entity 13: (71,72)\n",
        "Entity 14: (99,100)\n",
        "Entity 15: (105,108)\n",
        "Entity 16: (125,126) (134,134) (141,141)\n",
        "Entity 17: (147,148)\n",
        "Entity 18: (153,153)\n",
        "Entity 19: (161,163)\n",
        "Entity 20: (171,173)\n",
        "(__); __:\n",
        "Repeated mention in the key: 39, 43 1314\n",
        "Repeated mention in the key: 39, 43 1416\n",
        "Repeated mention in the response: 39, 43 1616\n",
        "Repeated mention in the response: 39, 43 1616\n",
        "Total key mentions: 29\n",
        "Total response mentions: 29\n",
        "Strictly correct identified mentions: 29\n",
        "Partially correct identified mentions: 0\n",
        "No identified: 0\n",
        "Invented: 2\n",
        "Recall: (9 / 10) 90%\tPrecision: (9 / 9) 100%\tF1: 94.73%\n",
        "--------------------------------------------------------------------------\n",
        "\n",
        "====== TOTALS =======\n",
        "Identification of Mentions: Recall: (29 / 29) 100%\tPrecision: (29 / 29) 100%\tF1: 100%\n",
        "--------------------------------------------------------------------------\n",
        "Coreference: Recall: (9 / 10) 90%\tPrecision: (9 / 9) 100%\tF1: 94.73%\n",
        "--------------------------------------------------------------------------\n",
        "\n",
        "/tmp/maz-14590.mmax.conll\n",
        "version: 8.01 /home/arne/repos/reference-coreference-scorers/lib/CorScorer.pm\n",
        "====> (__); __:\n",
        "File (__); __:\n",
        "Entity 0: (4,5)\n",
        "Entity 1: (6,8) (11,16) (106,107)\n",
        "Entity 2: (18,19)\n",
        "Entity 3: (26,29)\n",
        "Entity 4: (34,35) (77,80) (135,136)\n",
        "Entity 5: (37,46) (53,53)\n",
        "Entity 6: (37,103) (116,116) (121,121)\n",
        "Entity 7: (43,46)\n",
        "Entity 8: (44,46)\n",
        "Entity 9: (46,46) (59,59) (73,73) (90,91) (109,110)\n",
        "Entity 10: (53,73)\n",
        "Entity 11: (55,60)\n",
        "Entity 12: (55,65)\n",
        "Entity 13: (55,73) (85,87)\n",
        "Entity 14: (55,73)\n",
        "Entity 15: (55,69)\n",
        "Entity 16: (58,60)\n",
        "Entity 17: (79,80)\n",
        "Entity 18: (94,103)\n",
        "Entity 19: (100,103)\n",
        "Entity 20: (101,103)\n",
        "Entity 21: (130,131)\n",
        "====> (__); __:\n",
        "File (__); __:\n",
        "Entity 0: (4,5)\n",
        "Entity 1: (6,8) (11,16) (106,107)\n",
        "Entity 2: (18,19)\n",
        "Entity 3: (26,29)\n",
        "Entity 4: (34,35) (77,80) (135,136)\n",
        "Entity 5: (37,46) (53,53)\n",
        "Entity 6: (37,103) (116,116) (121,121)\n",
        "Entity 7: (43,46)\n",
        "Entity 8: (44,46)\n",
        "Entity 9: (46,46) (59,59) (73,73) (90,91) (109,110)\n",
        "Entity 10: (53,73)\n",
        "Entity 11: (55,60)\n",
        "Entity 12: (55,65)\n",
        "Entity 13: (55,73) (85,87)\n",
        "Entity 14: (55,73)\n",
        "Entity 15: (55,69)\n",
        "Entity 16: (58,60)\n",
        "Entity 17: (79,80)\n",
        "Entity 18: (94,103)\n",
        "Entity 19: (100,103)\n",
        "Entity 20: (101,103)\n",
        "Entity 21: (130,131)\n",
        "(__); __:\n",
        "Repeated mention in the key: 55, 73 2426\n",
        "Repeated mention in the response: 55, 73 2626\n",
        "Total key mentions: 33\n",
        "Total response mentions: 33\n",
        "Strictly correct identified mentions: 33\n",
        "Partially correct identified mentions: 0\n",
        "No identified: 0\n",
        "Invented: 1\n",
        "Recall: (11 / 12) 91.66%\tPrecision: (11 / 12) 91.66%\tF1: 91.66%\n",
        "--------------------------------------------------------------------------\n",
        "\n",
        "====== TOTALS =======\n",
        "Identification of Mentions: Recall: (33 / 33) 100%\tPrecision: (33 / 33) 100%\tF1: 100%\n",
        "--------------------------------------------------------------------------\n",
        "Coreference: Recall: (11 / 12) 91.66%\tPrecision: (11 / 12) 91.66%\tF1: 91.66%\n",
        "--------------------------------------------------------------------------\n",
        "\n",
        "/tmp/maz-17953.mmax.conll\n",
        "version: 8.01 /home/arne/repos/reference-coreference-scorers/lib/CorScorer.pm\n",
        "====> (__); __:\n",
        "File (__); __:\n",
        "Entity 0: (4,18)\n",
        "Entity 1: (4,7)\n",
        "Entity 2: (6,7)\n",
        "Entity 3: (6,18) (178,181)\n",
        "Entity 4: (9,12)\n",
        "Entity 5: (11,12)\n",
        "Entity 6: (14,18)\n",
        "Entity 7: (16,18)\n",
        "Entity 8: (20,23) (28,28) (61,61) (66,66) (113,115) (122,122) (164,164) (169,169) (177,177) (179,179) (196,196)\n",
        "Entity 9: (22,23)\n",
        "Entity 10: (24,25)\n",
        "Entity 11: (30,52) (159,161)\n",
        "Entity 12: (35,37)\n",
        "Entity 13: (44,48)\n",
        "Entity 14: (46,48)\n",
        "Entity 15: (49,52)\n",
        "Entity 16: (73,73)\n",
        "Entity 17: (80,89)\n",
        "Entity 18: (84,86) (97,98) (107,107)\n",
        "Entity 19: (84,89)\n",
        "Entity 20: (84,89) (102,105)\n",
        "Entity 21: (92,93)\n",
        "Entity 22: (155,162)\n",
        "Entity 23: (190,192) (198,200)\n",
        "Entity 24: (192,192)\n",
        "====> (__); __:\n",
        "File (__); __:\n",
        "Entity 0: (4,18)\n",
        "Entity 1: (4,7)\n",
        "Entity 2: (6,7)\n",
        "Entity 3: (6,18) (178,181)\n",
        "Entity 4: (9,12)\n",
        "Entity 5: (11,12)\n",
        "Entity 6: (14,18)\n",
        "Entity 7: (16,18)\n",
        "Entity 8: (20,23) (28,28) (61,61) (66,66) (113,115) (122,122) (164,164) (169,169) (177,177) (179,179) (196,196)\n",
        "Entity 9: (22,23)\n",
        "Entity 10: (24,25)\n",
        "Entity 11: (30,52) (159,161)\n",
        "Entity 12: (35,37)\n",
        "Entity 13: (44,48)\n",
        "Entity 14: (46,48)\n",
        "Entity 15: (49,52)\n",
        "Entity 16: (73,73)\n",
        "Entity 17: (80,89)\n",
        "Entity 18: (84,86) (97,98) (107,107)\n",
        "Entity 19: (84,89)\n",
        "Entity 20: (84,89) (102,105)\n",
        "Entity 21: (92,93)\n",
        "Entity 22: (155,162)\n",
        "Entity 23: (190,192) (198,200)\n",
        "Entity 24: (192,192)\n",
        "(__); __:\n",
        "Repeated mention in the key: 84, 89 3334\n",
        "Repeated mention in the response: 84, 89 3434\n",
        "Total key mentions: 40\n",
        "Total response mentions: 40\n",
        "Strictly correct identified mentions: 40\n",
        "Partially correct identified mentions: 0\n",
        "No identified: 0\n",
        "Invented: 1\n",
        "Recall: (15 / 16) 93.75%\tPrecision: (15 / 15) 100%\tF1: 96.77%\n",
        "--------------------------------------------------------------------------\n",
        "\n",
        "====== TOTALS =======\n",
        "Identification of Mentions: Recall: (40 / 40) 100%\tPrecision: (40 / 40) 100%\tF1: 100%\n",
        "--------------------------------------------------------------------------\n",
        "Coreference: Recall: (15 / 16) 93.75%\tPrecision: (15 / 15) 100%\tF1: 96.77%\n",
        "--------------------------------------------------------------------------\n",
        "\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/tmp/maz-9884.mmax.conll\n",
        "version: 8.01 /home/arne/repos/reference-coreference-scorers/lib/CorScorer.pm\n",
        "====> (__); __:\n",
        "File (__); __:\n",
        "Entity 0: (4,4)\n",
        "Entity 1: (13,14)\n",
        "Entity 2: (17,18)\n",
        "Entity 3: (24,27) (33,33) (37,37) (151,154) (163,163)\n",
        "Entity 4: (24,25) (151,152) (180,180)\n",
        "Entity 5: (24,27) (151,154) (188,188)\n",
        "Entity 6: (36,38)\n",
        "Entity 7: (42,47)\n",
        "Entity 8: (44,47)\n",
        "Entity 9: (50,75)\n",
        "Entity 10: (55,56) (63,63) (142,143) (144,144)\n",
        "Entity 11: (63,64)\n",
        "Entity 12: (65,65)\n",
        "Entity 13: (68,72)\n",
        "Entity 14: (71,72)\n",
        "Entity 15: (79,79) (86,86)\n",
        "Entity 16: (80,82)\n",
        "Entity 17: (93,94)\n",
        "Entity 18: (98,98)\n",
        "Entity 19: (100,103)\n",
        "Entity 20: (102,103)\n",
        "Entity 21: (105,105) (111,111)\n",
        "Entity 22: (106,116)\n",
        "Entity 23: (117,130)\n",
        "Entity 24: (124,127)\n",
        "Entity 25: (128,129)\n",
        "Entity 26: (131,131) (202,202)\n",
        "Entity 27: (131,135) (168,194)\n",
        "Entity 28: (133,133) (204,204)\n",
        "Entity 29: (135,135)\n",
        "Entity 30: (144,145)\n",
        "Entity 31: (148,154)\n",
        "Entity 32: (166,194)\n",
        "Entity 33: (172,173)\n",
        "Entity 34: (182,182) (184,184) (200,200)\n",
        "Entity 35: (183,185)\n",
        "Entity 36: (189,190)\n",
        "Entity 37: (191,193)\n",
        "Entity 38: (200,204) (219,219)\n",
        "Entity 39: (206,207)\n",
        "Entity 40: (221,224) (235,236)\n",
        "Entity 41: (230,230)\n",
        "Entity 42: (231,231)\n",
        "Entity 43: (241,243)\n",
        "====> (__); __:\n",
        "File (__); __:\n",
        "Entity 0: (4,4)\n",
        "Entity 1: (13,14)\n",
        "Entity 2: (17,18)\n",
        "Entity 3: (24,27) (33,33) (37,37) (151,154) (163,163)\n",
        "Entity 4: (24,25) (151,152) (180,180)\n",
        "Entity 5: (24,27) (151,154) (188,188)\n",
        "Entity 6: (36,38)\n",
        "Entity 7: (42,47)\n",
        "Entity 8: (44,47)\n",
        "Entity 9: (50,75)\n",
        "Entity 10: (55,56) (63,63) (142,143) (144,144)\n",
        "Entity 11: (63,64)\n",
        "Entity 12: (65,65)\n",
        "Entity 13: (68,72)\n",
        "Entity 14: (71,72)\n",
        "Entity 15: (79,79) (86,86)\n",
        "Entity 16: (80,82)\n",
        "Entity 17: (93,94)\n",
        "Entity 18: (98,98)\n",
        "Entity 19: (100,103)\n",
        "Entity 20: (102,103)\n",
        "Entity 21: (105,105) (111,111)\n",
        "Entity 22: (106,116)\n",
        "Entity 23: (117,130)\n",
        "Entity 24: (124,127)\n",
        "Entity 25: (128,129)\n",
        "Entity 26: (131,131) (202,202)\n",
        "Entity 27: (131,135) (168,194)\n",
        "Entity 28: (133,133) (204,204)\n",
        "Entity 29: (135,135)\n",
        "Entity 30: (144,145)\n",
        "Entity 31: (148,154)\n",
        "Entity 32: (166,194)\n",
        "Entity 33: (172,173)\n",
        "Entity 34: (182,182) (184,184) (200,200)\n",
        "Entity 35: (183,185)\n",
        "Entity 36: (189,190)\n",
        "Entity 37: (191,193)\n",
        "Entity 38: (200,204) (219,219)\n",
        "Entity 39: (206,207)\n",
        "Entity 40: (221,224) (235,236)\n",
        "Entity 41: (230,230)\n",
        "Entity 42: (231,231)\n",
        "Entity 43: (241,243)\n",
        "(__); __:\n",
        "Repeated mention in the key: 24, 27 311\n",
        "Repeated mention in the key: 151, 154 612\n",
        "Repeated mention in the response: 24, 27 1111\n",
        "Repeated mention in the response: 151, 154 1212\n",
        "Total key mentions: 62\n",
        "Total response mentions: 62\n",
        "Strictly correct identified mentions: 62\n",
        "Partially correct identified mentions: 0\n",
        "No identified: 0\n",
        "Invented: 2\n",
        "Recall: (17 / 20) 85%\tPrecision: (17 / 18) 94.44%\tF1: 89.47%\n",
        "--------------------------------------------------------------------------\n",
        "\n",
        "====== TOTALS =======\n",
        "Identification of Mentions: Recall: (62 / 62) 100%\tPrecision: (62 / 62) 100%\tF1: 100%\n",
        "--------------------------------------------------------------------------\n",
        "Coreference: Recall: (17 / 20) 85%\tPrecision: (17 / 18) 94.44%\tF1: 89.47%\n",
        "--------------------------------------------------------------------------\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 87
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Are the coreferences of the remaining files okay?\n",
      "\n",
      "- they seem okay, but contain numerous near-identity relations"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_all_good_scoring_files(mmax_dir, scorer_path, verbose=False):\n",
      "    \"\"\"\n",
      "    returns filepaths of MMAX2 coreference files which don't produce perfect\n",
      "    results when testing them against themselves with scorer.pl\n",
      "    \"\"\"\n",
      "    all_mmax_files = glob.glob(os.path.join(mmax_dir, '*.mmax'))\n",
      "    all_bad_files = set()\n",
      "    metrics = ['muc', 'bcub', 'ceafm', 'ceafe', 'blanc']\n",
      "    \n",
      "    for mmax_file in all_mmax_files:\n",
      "        for metric in metrics:\n",
      "            if not has_valid_annotation(mmax_file, scorer_path, metric, verbose=verbose):\n",
      "                all_bad_files.add(mmax_file)\n",
      "                break # continue with next mmax file\n",
      "    \n",
      "    return set(all_mmax_files).difference(all_bad_files)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_good_scoring_files = get_all_good_scoring_files(MMAX_DIR, SCORER_PATH)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(all_good_scoring_files)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 90,
       "text": [
        "156"
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# for fname in all_good_scoring_files:\n",
      "#     mdg = dg.read_mmax2(fname)\n",
      "#     bname = os.path.basename(fname)\n",
      "#     print bname, '\\n==============\\n\\n'\n",
      "#     try:\n",
      "#         # [the dog]_{markable_23} means that [the dog] is part of a\n",
      "#         # coreference chain whose first element is markable_23\n",
      "#         print dg.readwrite.brackets.gen_bracketed_output(mdg), '\\n\\n'\n",
      "#     except KeyError as e:\n",
      "#         print \"Error in {}: {}\".format(bname, e)\n",
      "#         print dg.get_text(mdg)\n",
      "#     try:\n",
      "#         print_all_chains(mdg)\n",
      "#     except Exception as e:\n",
      "#         print \"\\n{} FAILED: {}\".format(fname, e)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 91
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 91
    }
   ],
   "metadata": {}
  }
 ]
}